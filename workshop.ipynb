{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genki-BOT Secure API Workshop\n",
    "Welcome to this workshop that will teach you the basics of working with the Genki-BOT API via the OpenAI Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01: Setting up the client, selecting a model and generating your first response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing the OpenAI SDK**\n",
    "\n",
    "To get startet, the first thing we have to do is to install the openai python package.\n",
    "\n",
    "We do this by running the pip package installer in terminal, or like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q openai #q is for quiet mode, so we don't see the output of the installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to import the AzureOpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the API, we have to instantiate a client object.\n",
    "\n",
    "The client object need 3 inputs:\n",
    "- **api_key**: Your personal API key generated via the Genki-BOT API interface at https://genki-bot.ffdb.com/api-keys\n",
    "- **api_version**: Any supported API version from Microsoft, refer to Microsoft for the latest overview: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-lifecycle .\n",
    "- **azure_endpoint**: For this workshop: \"https://ca-gateway-genki-bot-dev.whitehill-48675d4c.swedencentral.azurecontainerapps.io\", an updated production URL will be available to view from within the Genki-BOT API interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=\"insert-your-generated-genki-key\",  ## INSERT YOUR KEY FROM GENKI\n",
    "    api_version=\"2025-03-01-preview\",\n",
    "    azure_endpoint=\"https://ca-gateway-genki-bot-dev.whitehill-48675d4c.swedencentral.azurecontainerapps.io\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The openai client supports a few different API structures, the most recent is called Responses API which is a recent replacement for the so calle Completion API. We will use this throughout the examples. \n",
    "\n",
    "Azure docs on Responses API: https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/responses?tabs=python-key\n",
    "\n",
    "**Model support**\n",
    "In order to generate a response from a language model, we need to specify which model to use.\n",
    "\n",
    "Genki-BOT API currenty supports\n",
    "- General-purpose language models:\n",
    "    - gpt-4o\n",
    "    - gpt-4.1\n",
    "    - gpt-4o-mini\n",
    "- Reasoning-optimized models:\n",
    "    - o3\n",
    "    - o3-mini\n",
    "    - o4-mini\n",
    "- Embedding model for text similarity/search (these are not called via the Responses API):\n",
    "    - text-embedding-3-large\n",
    "\n",
    "**input**\n",
    "Secondly, we need to define the text input the model should respond to. The input generally follows a structure resembling a multi-turn conversation, where each input-pair is defined by a role ('user', 'system' or 'assistant') and the text content.\n",
    "- User is the role used for any general text you want processed, whether it's a document input or a question as is typically what you will use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the cat sit on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the mouse!\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\", # Replace with your model deployment name \n",
    "    input=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": \"Tell me a joke about cats\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "1. Change the text input and generate a new reply\n",
    "\n",
    "2. Change the model by selecting another language or reasoning model and generate a new reply (not embedding model)\n",
    "\n",
    "3. Inspect the response object, it contains more than just the response, add this line of code: `print(response.model_dump_json(indent=2))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bonus:* The response client also supports a 'system' role. This role should be put first, and defines the behavior of the AI model and will affect any response the model provides to the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the cat a great pharmacist?\n",
      "\n",
      "Because it always knew the purrfect dosage!\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a pharma nerd, you always provide answers with a pharma twist\"\n",
    "         },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Tell me a joke about cats\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02: Structured outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many use-cases we want the output of the language model to follow a strict format, and the output to be of a specific type (number, text, list etc.). \n",
    "\n",
    "By using structured outputs, we can define a schema for how we want the reply from the AI model to be, and it will adhere to this structure. In python, such structures is created via a 'class'. If we then pass in the class as the text_format attribute, we are certain that the output will follow this class structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incident date: 2024-05-15\n",
      "Invovled personel: ['Peter', 'Lisa']\n",
      "Incident description: Peter dropped the wrench into the instrument while working on the manufacturing site.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Deviation(BaseModel):\n",
    "    incident_date: str\n",
    "    invovled_personel: list[str]\n",
    "    incident_description: str\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o\", \n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are deviation analyst, you are given a description of an incident and you need to extract the information and return it in a structured way.\"\n",
    "        }\n",
    "        ,\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Peter and Lisa was working in the manufacturing site, when Peter dropped the wrench into the instrument. It happend Thursday the 15th of May 2024.\"\n",
    "        }   \n",
    "    ],\n",
    "    text_format=Deviation\n",
    ")\n",
    "\n",
    "incident = response.output_parsed\n",
    "print(f'Incident date: {incident.incident_date}')\n",
    "print(f'Invovled personel: {incident.invovled_personel}')\n",
    "print(f'Incident description: {incident.incident_description}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "1. Change the description of the deviation incident and run the code again [the content field with Peter and Lisa ...]\n",
    "2. Add a field to the Deviation class and re-run the response.\n",
    "3. Update the system text, how does this affect the outcome [the content field with 'You are a deviation ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03: Image inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multimodal models such as gpt-4o also support image inputs and can analyze and extract information from even fairly complex images. The Azure OpenAI API needs the images converted into a format called base64.\n",
    "\n",
    "Below is a simple example where the user input also includes an image from the FLB website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows an aerial view of a lush green forest with winding rivers. Overlaid on this scenery is text that reads \"Partners for the Planet\" alongside a globe-like design.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"PFP-Cover-Image-Sustainability.jpg\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"what's in this image?\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "1. Inspect the .jpg image, does the image allign with the output of the model?\n",
    "2. Replace the image_path name with `ai-generated-notes.jpg` and see whether the model can read handwritten text\n",
    "3. Change the input text to ensure that the AI extract the full text from the ai generated notes and prints those out\n",
    "4. If you have an image on your computer or phone [non confidential], try to upload this file, and have the model analyze the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04: Text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text embedding models are AI models trained to convert text into vectors, so that we can work mathematically with meaning and context. This is often usefull if we want to compare hundreds, thousands or even millions of text records by using efficient algorithms. This, is a core component of a RAG system, where the most naive retrieval technique simply use embeddings to retrieve relevant pieces of text.\n",
    "\n",
    "Let us first embed a single piece of text and inspect the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0007559562800452113, -0.006044063251465559, -0.009024844504892826, 0.01776272989809513, 5.85126290388871e-05, -0.0327419638633728, 0.009397890418767929, 0.02189493179321289, 0.013695093803107738, 0.0015513693215325475, -0.02716062031686306, -0.023860597983002663, -0.008931582793593407, -0.00921136699616909, -0.010825509205460548, 0.02123492769896984, 0.010337679646909237, -0.015337930992245674, -0.02281319908797741, 0.014204445295035839]\n"
     ]
    }
   ],
   "source": [
    "SOP_1_embedding = client.embeddings.create(\n",
    "    input=\"SOP 1: Hygiene and cleaning in the production area\",\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "print(SOP_1_embedding.data[0].embedding[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create 2 more embeddings. In order to compare how similar the embeddings are to each other, we can use the cosine similarity metric which is one of the most common metrics for this purpose.\n",
    "\n",
    "The cosine similarity score is in the range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between SOP 1 and SOP 2: 0.6562185395643199\n",
      "Similarity between SOP 1 and SOP 3: 0.3927057636338306\n",
      "Similarity between SOP 2 and SOP 3: 0.42579820373983357\n"
     ]
    }
   ],
   "source": [
    "SOP_2_embedding = client.embeddings.create(\n",
    "    input=\"SOP 2: Cleaning and disinfection of the storage facility\",\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "SOP_3_embedding = client.embeddings.create(\n",
    "    input=\"SOP 3: Training of employees on forklift operation\",\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# Calculate the cosine similarity between the embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity([SOP_1_embedding.data[0].embedding, \n",
    "                                       SOP_2_embedding.data[0].embedding, \n",
    "                                       SOP_3_embedding.data[0].embedding])\n",
    "\n",
    "print(f'Similarity between SOP 1 and SOP 2: {similarity_matrix[0][1]}')\n",
    "print(f'Similarity between SOP 1 and SOP 3: {similarity_matrix[0][2]}')\n",
    "print(f'Similarity between SOP 2 and SOP 3: {similarity_matrix[1][2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "1. Do the above similar scores match what you would expect?\n",
    "2. Modify the existing SOP input texts and see how the scores are effected, does the results match your intuition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuji",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
