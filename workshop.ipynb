{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genki-BOT Secure API Workshop\n",
    "Welcome to this workshop that will teach you the basics of working with the Genki-BOT API via the OpenAI Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01: Setting up the client, selecting a model and generating your first response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing the OpenAI SDK**\n",
    "\n",
    "To get startet, the first thing we have to do is to install the openai python package.\n",
    "\n",
    "We do this by running the pip package installer in terminal, or like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q openai #q is for quiet mode, so we don't see the output of the installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to import the AzureOpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the API, we have to instantiate a client object.\n",
    "\n",
    "The client object need 3 inputs:\n",
    "- **api_key**: Your personal API key generated via the Genki-BOT API interface at https://genki-bot.ffdb.com/api-keys\n",
    "- **api_version**: Any supported API version from Microsoft, refer to Microsoft for the latest overview: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-lifecycle .\n",
    "- **azure_endpoint**: For this workshop: \"https://ca-gateway-genki-bot-dev.whitehill-48675d4c.swedencentral.azurecontainerapps.io\", an updated production URL will be available to view from within the Genki-BOT API interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=\"insert-your-key-here\",  ## INSERT YOUR KEY FROM GENKI\n",
    "    api_version=\"2025-03-01-preview\",\n",
    "    azure_endpoint=\"https://ca-gateway-genki-bot-dev.whitehill-48675d4c.swedencentral.azurecontainerapps.io\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The openai client supports a few different API structures, the most recent is called Responses API which is a recent replacement for the so calle Completion API. We will use this throughout the examples. \n",
    "\n",
    "Azure docs on Responses API: https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/responses?tabs=python-key\n",
    "\n",
    "**Model support**\n",
    "In order to generate a response from a language model, we need to specify which model to use.\n",
    "\n",
    "Genki-BOT API currenty supports\n",
    "- General-purpose language models:\n",
    "    - gpt-4o\n",
    "    - gpt-4.1\n",
    "    - gpt-4o-mini\n",
    "- Reasoning-optimized models:\n",
    "    - o3\n",
    "    - o3-mini\n",
    "    - o4-mini\n",
    "- Embedding model for text similarity/search (these are not called via the Responses API):\n",
    "    - text-embedding-3-large\n",
    "\n",
    "**input**\n",
    "Secondly, we need to define the text input the model should respond to. The input generally follows a structure resembling a multi-turn conversation, where each input-pair is defined by a role ('user', 'system' or 'assistant') and the text content.\n",
    "- User is the role used for any general text you want processed, whether it's a document input or a question as is typically what you will use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the cat sitting on the computer?\n",
      "\n",
      "It wanted to keep an eye on the mouse!\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\", # Replace with your model deployment name \n",
    "    input=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": \"Tell me a joke about cats\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "1. Change the text input and generate a new reply\n",
    "\n",
    "2. Change the model by selecting another language or reasoning model and generate a new reply (not embedding model)\n",
    "\n",
    "3. Inspect the response object, it contains more than just the response, add this line of code: `print(response.model_dump_json(indent=2))`\n",
    "\n",
    "4. The response client also supports a 'system' role. This role should be put first, and defines the behavior of the AI model and will affect any response the model provides to the input text following that.\n",
    "\n",
    "   Run the below cell and see the output. Now change the content of the system role, f.x. to \"Always translate the input to Japanese\", try with other system messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the cat sitting on the pharmacist's counter?\n",
      "\n",
      "Because it had a \"purr-scription\" that needed filling!\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a pharma nerd, you always provide answers with a pharma twist\"\n",
    "         },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Tell me a joke about cats\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02: Structured outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many use-cases we want the output of the language model to follow a strict format, and the output to be of a specific type (number, text, list etc.). \n",
    "\n",
    "By using structured outputs, we can define a schema for how we want the reply from the AI model to be, and it will adhere to this structure. In python, such structures is created via a 'class'. If we then pass in the class as the text_format attribute, we are certain that the output will follow this class structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incident date: 2024-05-15\n",
      "Invovled personel: ['Peter', 'Lisa']\n",
      "Incident description: Peter dropped the wrench into the instrument while working on the manufacturing site.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Deviation(BaseModel):\n",
    "    incident_date: str\n",
    "    invovled_personel: list[str]\n",
    "    incident_description: str\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o\", \n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are deviation analyst, you are given a description of an incident and you need to extract the information and return it in a structured way.\"\n",
    "        }\n",
    "        ,\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Peter and Lisa was working in the manufacturing site, when Peter dropped the wrench into the instrument. It happend Thursday the 15th of May 2024.\"\n",
    "        }   \n",
    "    ],\n",
    "    text_format=Deviation\n",
    ")\n",
    "\n",
    "incident = response.output_parsed\n",
    "print(f'Incident date: {incident.incident_date}')\n",
    "print(f'Invovled personel: {incident.invovled_personel}')\n",
    "print(f'Incident description: {incident.incident_description}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "1. Change the description of the deviation incident and run the code again [the content field with Peter and Lisa ...]\n",
    "2. Add a field to the Deviation class and re-run the response.\n",
    "3. Update the system text, how does this affect the outcome [the content field with 'You are a deviation ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03: Image inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multimodal models such as gpt-4o also support image inputs and can analyze and extract information from even fairly complex images. The Azure OpenAI API needs the images converted into a format called base64.\n",
    "\n",
    "Below is a simple example where the user input also includes an image from the FLB website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows an aerial view of a lush green forest with winding rivers. Overlaid on this scenery is text that reads \"Partners for the Planet\" alongside a globe-like design.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"PFP-Cover-Image-Sustainability.jpg\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"what's in this image?\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "1. Inspect the .jpg image, does the image allign with the output of the model?\n",
    "2. Replace the image_path name with `ai-generated-notes.jpg` and see whether the model can read handwritten text\n",
    "3. Change the input text to ensure that the AI extract the full text from the ai generated notes and prints those out\n",
    "4. If you have an image on your computer or phone [non confidential], try to upload this file, and have the model analyze the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04: Text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text embedding models are AI models trained to convert text into vectors, so that we can work mathematically with meaning and context. This is often usefull if we want to compare hundreds, thousands or even millions of text records by using efficient algorithms. This, is a core component of a RAG system, where the most naive retrieval technique simply use embeddings to retrieve relevant pieces of text.\n",
    "\n",
    "Let us first embed a single piece of text and inspect the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0007559562800452113, -0.006044063251465559, -0.009024844504892826, 0.01776272989809513, 5.85126290388871e-05, -0.0327419638633728, 0.009397890418767929, 0.02189493179321289, 0.013695093803107738, 0.0015513693215325475, -0.02716062031686306, -0.023860597983002663, -0.008931582793593407, -0.00921136699616909, -0.010825509205460548, 0.02123492769896984, 0.010337679646909237, -0.015337930992245674, -0.02281319908797741, 0.014204445295035839]\n"
     ]
    }
   ],
   "source": [
    "SOP_1_embedding = client.embeddings.create(\n",
    "    input=\"SOP 1: Hygiene and cleaning in the production area\",\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "print(SOP_1_embedding.data[0].embedding[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create 2 more embeddings. In order to compare how similar the embeddings are to each other, we can use the cosine similarity metric which is one of the most common metrics for this purpose.\n",
    "\n",
    "The cosine similarity score is in the range of 0 to 1.In order to easily calculate the cosine similarity between multiple pairs of embeddings, we will use the Sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between SOP 1 and SOP 2: 0.6562185395643199\n",
      "Similarity between SOP 1 and SOP 3: 0.3927057636338306\n",
      "Similarity between SOP 2 and SOP 3: 0.42579820373983357\n"
     ]
    }
   ],
   "source": [
    "SOP_2_embedding = client.embeddings.create(\n",
    "    input=\"SOP 2: Cleaning and disinfection of the storage facility\",\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "SOP_3_embedding = client.embeddings.create(\n",
    "    input=\"SOP 3: Training of employees on forklift operation\",\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# Calculate the cosine similarity between the embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity([SOP_1_embedding.data[0].embedding, \n",
    "                                       SOP_2_embedding.data[0].embedding, \n",
    "                                       SOP_3_embedding.data[0].embedding])\n",
    "\n",
    "print(f'Similarity between SOP 1 and SOP 2: {similarity_matrix[0][1]}')\n",
    "print(f'Similarity between SOP 1 and SOP 3: {similarity_matrix[0][2]}')\n",
    "print(f'Similarity between SOP 2 and SOP 3: {similarity_matrix[1][2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "1. Do the above similar scores match what you would expect?\n",
    "2. Modify the existing SOP input texts and see how the scores are effected, does the results match your intuition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05: Tool Usage & Function calling with Chat Completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 05.1 Why use Chat Completions for tools?\n",
    "\n",
    "- Chat Completions is mature, widely documented, and identical to the OpenAI global endpoint, so sample code “just works” on Azure. \n",
    "- The functions field lets the model decide when to call helper code, then finish the reply with your function’s JSON result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 05.2 Define the helper function & schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will create a helper function that can look up live weather data if the user's question implies the need for knowing a specific city's weather for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_current_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    Look up live weather via the free open-meteo.com REST API and\n",
    "    return a JSON string (because the Chat API expects a string payload).\n",
    "    \"\"\"\n",
    "    geo = requests.get(\n",
    "        \"https://geocoding-api.open-meteo.com/v1/search\",\n",
    "        params={\"name\": location, \"count\": 1},\n",
    "        timeout=10\n",
    "    ).json()\n",
    "\n",
    "    if not geo.get(\"results\"):\n",
    "        return json.dumps({\"error\": f\"Location '{location}' not found.\"})\n",
    "\n",
    "    lat, lon = geo[\"results\"][0][\"latitude\"], geo[\"results\"][0][\"longitude\"]\n",
    "    weather = requests.get(\n",
    "        \"https://api.open-meteo.com/v1/forecast\",\n",
    "        params={\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"current_weather\": True,\n",
    "            \"temperature_unit\": unit,\n",
    "        },\n",
    "        timeout=10\n",
    "    ).json()[\"current_weather\"]\n",
    "\n",
    "    return json.dumps({\n",
    "        **weather,\n",
    "        \"requested_location\": location,\n",
    "        \"retrieved_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"unit\": unit\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 05.3 Tool schema\n",
    "The schema mirrors OpenAI’s function-calling spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON schema that tells the model how to call the function\n",
    "weather_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",          # <-- required\n",
    "        \"description\": \"Return the current weather for a city.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country, e.g. 'Copenhagen, Denmark'\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"Temperature unit\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why the wrapper?**\n",
    "\n",
    "The service inspects tools[*].type. If the key or its \"name\" sub-property is missing you’ll get “Missing required parameter: 'tools[0].name'”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 05.4 Utility: run a chat with automatic function use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(prompt: str, deployment_name: str = \"gpt-4o\") -> str:\n",
    "    # STEP 1 ─ Let the model decide if it needs the tool\n",
    "    first = client.chat.completions.create(\n",
    "        model      = deployment_name,\n",
    "        messages   = [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tools      = [weather_tool],\n",
    "        tool_choice= \"auto\"\n",
    "    )\n",
    "\n",
    "    assistant_msg = first.choices[0].message\n",
    "\n",
    "    # If the model requested the function ...\n",
    "    if assistant_msg.tool_calls:\n",
    "        tool_call   = assistant_msg.tool_calls[0]\n",
    "        args        = json.loads(tool_call.function.arguments)\n",
    "        tool_result = get_current_weather(**args)\n",
    "\n",
    "        # STEP 2 ─ Send the tool result back so the model can finish\n",
    "        second = client.chat.completions.create(\n",
    "            model    = deployment_name,\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                assistant_msg,   # the function-call request\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"name\": tool_call.function.name,\n",
    "                    \"content\": tool_result\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return second.choices[0].message.content, tool_result\n",
    "\n",
    "    # No function needed\n",
    "    return assistant_msg.content, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-call flow is the exact sequence recommended by both OpenAI and Azure docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 05.5 Example A – Weather question (tool invoked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The current weather in Copenhagen is partly cloudy with a temperature of 17.8°C. There is no mention of rain at this moment, so you probably don't need an umbrella right now. However, weather conditions can change, so you might want to keep an umbrella handy, just in case.\n",
      "\n",
      "Tool result: {\"time\": \"2025-06-17T07:30\", \"interval\": 900, \"temperature\": 17.8, \"windspeed\": 15.1, \"winddirection\": 281, \"is_day\": 1, \"weathercode\": 3, \"requested_location\": \"Copenhagen, Denmark\", \"retrieved_at\": \"2025-06-17T07:39:51.732977Z\", \"unit\": \"celsius\"}\n"
     ]
    }
   ],
   "source": [
    "# Weather question – tool will be invoked\n",
    "answer, tool_result = ask_llm(\"I'm visiting Copenhagen today - do I need an umbrella?\")\n",
    "\n",
    "print(\"Model response:\\n\", answer)\n",
    "print(\"\\n\\nTool result:\", tool_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens behind the scenes?\n",
    "\n",
    "1. The model detects the user wants weather.\n",
    "2. It returns a tool call request with arguments {\"location\": \"Copenhagen, Denmark\"}.\n",
    "3. Your code (the SDK does this automatically in v1 preview) runs get_current_weather, gets the JSON, and sends a follow-up request containing the function output.\n",
    "4. The model writes the final answer, weaving the fresh weather into natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 05.6 Example B - Generic question (tool not involved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Why did the Danish pastry go to school?\n",
      "\n",
      "Because it wanted to be a little “butter” at everything!\n",
      "\n",
      "Tool result: None\n"
     ]
    }
   ],
   "source": [
    "# Non-weather question – tool is skipped\n",
    "answer, tool_result = ask_llm(\"Tell me a Danish pastry joke!\")\n",
    "\n",
    "\n",
    "print(\"Model response:\\n\", answer)\n",
    "print(\"\\n\\nTool result:\", tool_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the user’s request isn’t weather-related, the model simply replies without invoking the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 05.7 Things to try\n",
    "\n",
    "1. Change the location or set \"unit\": \"fahrenheit\" and observe the tool arguments.\n",
    "2. Add error handling – ask for an imaginary city and see the graceful fallback.\n",
    "3. Chain tools – combine the weather data with a packing-list generator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuji",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
